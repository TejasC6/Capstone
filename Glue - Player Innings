import boto3

get_last_modified = lambda obj: int(obj['LastModified'].strftime('%s'))

s3 = boto3.client('s3')
objs = s3.list_objects_v2(Bucket='<input_bucket_name>')['Contents']
last_added = [obj['Key'] for obj in sorted(objs, key=get_last_modified)][-1]


print('all objs', objs)
print('last added', last_added)


import sys
import boto3
import pandas as pd
import numpy as np
import warnings
from io import StringIO

warnings.filterwarnings("ignore")

#connecting inot s3
def s3_connection():
    session = boto3.Session()
    s3 = boto3.client('s3')
    return s3

def fetch_data_s3(input_bucket,input_file):
# bucket name
    s3 = s3_connection()
    ### reading csv file into dataframe
    obj = s3.get_object(Bucket = input_bucket, Key = input_file).get('Body').read().decode('utf-8')
    data = StringIO(obj)
    df = pd.read_csv(data)
    return df
#calling clean data function

## saving dataframe as csv file in s3
def load_data_s3(df, output_bucket,output_file):
    s3 = s3_connection()
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    content = csv_buffer.getvalue()
    s3_resource = boto3.resource('s3')
    s3_resource.Object(output_bucket, output_file).put(Body=csv_buffer.getvalue()) # saving csv in s3 bucket
    return 'Successful'
 
 
 
 #=============================================
 ## data cleaning functions
 
 ### Player_Innings
def clean_player_innings(df):
    
    list_directories = last_added.split('/')
    df.drop_duplicates()
    df.replace('-', np.nan, inplace=True)
    df.replace('TDNB','DNB',inplace=True)
    df.drop(columns=['Innings Runs Scored','Innings Runs Scored Buckets','Innings Wickets Taken Buckets'], inplace=True)
    df['gender'] = list_directories[0]
    df['year'] = pd.DatetimeIndex(df['Innings Date']).year
        
    if (df.year[0] >= 1801 and df.year[0] <= 1900):
            df['century'] = '19th'
    elif(df.year[0] >= 1901 and df.year[0] <= 2000):
            df['century'] = '20th'
    elif(df.year[0] >= 2001 and df.year[0] <= 2100):
            df['century'] = '21st'
    else:
            df['century'] = '22nd'
            
    df['format'] = list_directories[1]
    
    df.drop(columns = ['year'], inplace = True)    
    print('all columns in cleaned df now are: ', df.head())
    df.rename(columns={'Innings Player':'player','Innings Runs Scored Num':'runs_scored','Innings Minutes Batted':'minutes_batted','Innings Batted Flag':'batted_flag','Innings Not Out Flag':'not_out_flag','Innings Balls Faced':'balls_faced','Innings Boundary Fours':'fours','Innings Boundary Sixes':'sixes','Innings Batting Strike Rate':'strike_rate','Innings Number':'innings_number','Opposition':'opposition','Ground':'ground','Innings Date':'date','Country':'country',"50's":'fifty',"100's":'hundred','Innings Overs Bowled':'overs_bowled','Innings Bowled Flag':'bowled_flag','Innings Maidens Bowled':'maidens_overs','Innings Runs Conceded':'runs_conceded','Innings Wickets Taken':'wickets_taken','4 Wickets':'four_wicket_haul','5 Wickets':'five_wicket_haul','10 Wickets':'ten_wicket_haul','Innings Economy Rate':'economy_rate', 'gender': 'gender', 'century': 'century', 'format':'format'}, inplace = True)
    return df
# delete object or file in s3    
def delete_data_s3(bucket_name,file_name):
    s3 = s3_connection()
    s3.delete_object(Bucket = bucket_name, Key = file_name)

# merge data into exisitng csv file
def merge(df1,df2):
    
    df = pd.concat([df1,df2], join='outer')
    return df

# Player_Innings file transform
def transform_ipl(input_bucket, input_file, output_bucket, output_file):
    s3 = s3_connection()
    df = fetch_data_s3(input_bucket,input_file)
    print('fetched this data from input bucket', df)
    df = clean_player_innings(df)
    print('after cleaning fetched data from input bucket', df)
    df1 = fetch_data_s3(output_bucket,output_file)
    print('fetched this from output bucket', df1)
    delete_data_s3(output_bucket, output_file)
    print('deleted previous file in output bucket')
    base_df = merge(df1,df)
    print('merged cleaned data from input to previously cleaned data from output', base_df)
    load_data_s3(base_df,output_bucket,output_file)
    print('full old+new cleaned data loaded back to output bucket ')
    
    return 'Success'

transform_ipl('<input_bucket_name>', last_added,'<output_bucket_name>', '<output_file_name.csv>')
